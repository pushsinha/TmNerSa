{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "#from pyspark.mllib.clustering import LDA\n",
    "from pyspark.mllib.clustering import LDA, LDAModel\n",
    "#from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import *\n",
    "import re \n",
    "import struct\n",
    "#import pyLDAvis\n",
    "from struct import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StringType, ArrayType\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer, CountVectorizer, HashingTF, IDF\n",
    "from array import array\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "#pyspark.sql.types.StructType,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\VA Lab\n",
      "[nltk_data]     11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\VA Lab\n",
      "[nltk_data]     11\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "lemma = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "         .appName(\"spark-nltk\") \\\n",
    "         .getOrCreate()\n",
    "\n",
    "data = spark.read.csv(\"../../SOCC/raw/gnm_comments.csv\", header=True) #gives a dataframe\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comment_text = data.select('comment_text')\n",
    "comment_text=comment_text.replace(r'\\\\n\\\\n|\\\\n',' ')\n",
    "comment_text=comment_text.replace(r\"\"\"(?i)\\b((?:https?:(?:/{1,3}|[a-z0-9%])|[a-z0-9.\\-]+[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)/)(?:[^\\s()<>{}\\[\\]]+|\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\))+(?:\\([^\\s()]*?\\([^\\s()]+\\)[^\\s()]*?\\)|\\([^\\s]+?\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’])|(?:(?<!@)[a-z0-9]+(?:[.\\-][a-z0-9]+)*[.](?:com|net|org|edu|gov|mil|aero|asia|biz|cat|coop|info|int|jobs|mobi|museum|name|post|pro|tel|travel|xxx|ac|ad|ae|af|ag|ai|al|am|an|ao|aq|ar|as|at|au|aw|ax|az|ba|bb|bd|be|bf|bg|bh|bi|bj|bm|bn|bo|br|bs|bt|bv|bw|by|bz|ca|cc|cd|cf|cg|ch|ci|ck|cl|cm|cn|co|cr|cs|cu|cv|cx|cy|cz|dd|de|dj|dk|dm|do|dz|ec|ee|eg|eh|er|es|et|eu|fi|fj|fk|fm|fo|fr|ga|gb|gd|ge|gf|gg|gh|gi|gl|gm|gn|gp|gq|gr|gs|gt|gu|gw|gy|hk|hm|hn|hr|ht|hu|id|ie|il|im|in|io|iq|ir|is|it|je|jm|jo|jp|ke|kg|kh|ki|km|kn|kp|kr|kw|ky|kz|la|lb|lc|li|lk|lr|ls|lt|lu|lv|ly|ma|mc|md|me|mg|mh|mk|ml|mm|mn|mo|mp|mq|mr|ms|mt|mu|mv|mw|mx|my|mz|na|nc|ne|nf|ng|ni|nl|no|np|nr|nu|nz|om|pa|pe|pf|pg|ph|pk|pl|pm|pn|pr|ps|pt|pw|py|qa|re|ro|rs|ru|rw|sa|sb|sc|sd|se|sg|sh|si|sj|Ja|sk|sl|sm|sn|so|sr|ss|st|su|sv|sx|sy|sz|tc|td|tf|tg|th|tj|tk|tl|tm|tn|to|tp|tr|tt|tv|tw|tz|ua|ug|uk|us|uy|uz|va|vc|ve|vg|vi|vn|vu|wf|ws|ye|yt|yu|za|zm|zw)\\b/?(?!@)))\"\"\",' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanup_text(text):\n",
    "    \n",
    "    exclude = set(string.punctuation)\n",
    "    translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "    no_punct = text.lower().translate(translate_table)\n",
    "    words = no_punct.split()\n",
    "    #words = translate_table\n",
    "    # Default list of Stopwords\n",
    "    stopwords_core = ['a', u'about', u'above', u'after', u'again', u'against', u'all', u'am', u'an', u'and', u'any', u'are', u'arent', u'as', u'at', \n",
    "    u'be', u'because', u'been', u'before', u'being', u'below', u'between', u'both', u'but', u'by', \n",
    "    u'can', 'cant', 'come', u'could', 'couldnt', \n",
    "    u'd', u'did', u'didn', u'do', u'does', u'doesnt', u'doing', u'dont', u'down', u'during', \n",
    "    u'each', \n",
    "    u'few', 'finally', u'for', u'from', u'further', \n",
    "    u'had', u'hadnt', u'has', u'hasnt', u'have', u'havent', u'having', u'he', u'her', u'here', u'hers', u'herself', u'him', u'himself', u'his', u'how', \n",
    "    u'i', u'if', u'in', u'into', u'is', u'isnt', u'it', u'its', u'itself', \n",
    "    u'just', \n",
    "    u'll', \n",
    "    u'm', u'me', u'might', u'more', u'most', u'must', u'my', u'myself', \n",
    "    u'no', u'nor', u'not', u'now', \n",
    "    u'o', u'of', u'off', u'on', u'once', u'only', u'or', u'other', u'our', u'ours', u'ourselves', u'out', u'over', u'own', \n",
    "    u'r', u're', \n",
    "    u's', 'said', u'same', u'she', u'should', u'shouldnt', u'so', u'some', u'such', \n",
    "    u't', u'than', u'that', 'thats', u'the', u'their', u'theirs', u'them', u'themselves', u'then', u'there', u'these', u'they', u'this', u'those', u'through', u'to', u'too', \n",
    "    u'under', u'until', u'up', \n",
    "    u'very', \n",
    "    u'was', u'wasnt', u'we', u'were', u'werent', u'what', u'when', u'where', u'which', u'while', u'who', u'whom', u'why', u'will', u'with', u'wont', u'would', \n",
    "    u'y', u'you', u'your', u'yours', u'yourself', u'yourselves']\n",
    "    \n",
    "    # Custom List of Stopwords - Add your own here\n",
    "    stopwords_custom = ['per', u'one', u'tell', u'need', u'say', u'new', u'try', u'take', u'may', u'come', u'get', \n",
    "    u'two', u'three', u'get', u'would', u'seem', u'want', u'hey', u'might', u'may', u'without', u'with', u'also', \n",
    "    u'make', u'want', u'put', u'etc', u'actually', u'else', u'far', u'definitely', u'youll', u'didnt', u'isnt', \n",
    "    u'theres', u'since', u'able', u'maybe', u'sort', u'think', u'know', u'look', u'please', u'one', u'null', u'dont',\n",
    "    u'could', u'unable', u'someday', u'someone', u'also', u'anyone', u'really',\n",
    "    u'something', u'give', u'years', u'use', u'all', u'ago', u'right', u'call', u'include', u'part', u'find',\n",
    "    u'become', u'choose', u'chosen', u'as', u'back', u'see', u'even',u'first', u'another', u'mine',\n",
    "    u'instead', u'will', u'never', u'ask', u'even', u'see', u'allow', u'still', u'that', u'you',\n",
    "    u'obviously', u'self', u'bye', u'well', u'make', u'take', u'let',\n",
    "    u'get', u'leave', u'live', u'say', u'tell', u'understand', u'look', u'seem',\n",
    "    u'nothing', u'everything', u'give', u'long', u'think', u'show', u'last', u'run', u'day', \n",
    "    u'try', u'yes', u'no', u'live', u'right', u'perhaps', u'already', u'never', u'ever', u'just',   \n",
    "    u'rather', u'however', u'real', u'bring', u'other', u'another', u'away', u'youre',     \n",
    "    u'enough', u'want', u'mine', u'yours', u'hear', u'either', u'nor', u'neither', u'look', u'however',\n",
    "    u'know', u'come', u'without', u'least', u'nah', u'bye', u'told'\n",
    "    u'often', u'anything', u'wrong', u'though', u'always', u'every', u'around', u'yet']\n",
    "    \n",
    "    stopwords = stopwords_core + stopwords_custom\n",
    "    stopwords = [word.lower() for word in stopwords]    \n",
    "    \n",
    "    normalized = [lemma.lemmatize(word,'v') for word in words]\n",
    "    text_out = [re.sub('[^a-zA-Z0-9]','',word) for word in normalized]                                       # Remove special characters\n",
    "    text_out = [word.lower() for word in text_out if len(word)>2 and word.lower() not in stopwords]     # Remove stopwords and words under X length\n",
    "    \n",
    "    #normalized = \" \".join(lemma.lemmatize(word,'v') for word in words)\n",
    "    text_out = \" \".join(lemma.lemmatize(word,'v') for word in text_out)\n",
    " \n",
    "\n",
    "    #x = normalized.split()\n",
    "    #y = [s for s in x if len(s) > 2]\n",
    "    \n",
    "    return text_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udf_cleantext = udf(cleanup_text, StringType())\n",
    "#print(data.comment_text)\n",
    "#comment_text.select(\"comment_text\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          clean_comm|\n",
      "+--------------------+\n",
      "|program work prob...|\n",
      "|offshoring revers...|\n",
      "|spell exploitatio...|\n",
      "+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clean_text = data.withColumn(\"clean_comm\", udf_cleantext(data.comment_text))\n",
    "clean_text.select(\"clean_comm\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"clean_comm\", outputCol=\"tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|          clean_comm|              tokens|\n",
      "+--------------------+--------------------+\n",
      "|program work prob...|[program, work, p...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenized = tokenizer.transform(clean_text)\n",
    "e = tokenized.select(\"clean_comm\", \"tokens\")\n",
    "tokenized.select(\"clean_comm\", \"tokens\").show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"tokens_Stop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         tokens_Stop|\n",
      "+--------------------+\n",
      "|[program, work, p...|\n",
      "|[offshoring, reve...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "t = remover.transform(e)\n",
    "tt = t.select('tokens_Stop')\n",
    "tt.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tt.toPandas().to_csv('../../SOCC/cleaned_comments_for_sentiment_analysis.csv')  #-Uncomment if oyu want to save a cleaned model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
